{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import networks as nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "unet = nw.UNet(input_size=(256, 256, 3), num_classes=1)\n",
    "\n",
    "out = unet.call(tf.random.normal((1, 256, 256, 3)))  # Call the model with a random input to build the model\n",
    "print(out.shape) # Make sure the output shape is correct\n",
    "\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Load an image, decode it, and resize to 256x256\"\"\"\n",
    "    img = tf.io.read_file(image_path)               # Read image\n",
    "    img = tf.image.decode_png(img, channels=3)       # Decode as PNG (assuming PNG images)\n",
    "    img = tf.image.resize(img, (256, 256))           # Resize to target size\n",
    "    img = img / 255.0                               # Normalize to range [0, 1]\n",
    "    return img\n",
    "\n",
    "def load_mask(mask_path):\n",
    "    \"\"\"Load a mask, decode it, and resize to 256x256\"\"\"\n",
    "    mask = tf.io.read_file(mask_path)               # Read mask image\n",
    "    mask = tf.image.decode_png(mask, channels=1)     # Decode as PNG (assuming masks are single-channel)\n",
    "    mask = tf.image.resize(mask, (256, 256))         # Resize to target size\n",
    "    mask = mask / 255.0                             # Normalize to range [0, 1]\n",
    "    return mask\n",
    "\n",
    "def load_data(image_folder, mask_folder):\n",
    "    # Get image and mask file paths\n",
    "    image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder)]\n",
    "    mask_paths = [os.path.join(mask_folder, fname) for fname in os.listdir(mask_folder)]\n",
    "    \n",
    "    # Shuffle the paths (optional)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
    "\n",
    "    # Load images and masks\n",
    "    dataset = dataset.map(lambda x, y: (load_image(x), load_mask(y)))\n",
    "    \n",
    "    # Shuffle and batch the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=1000)  # Shuffle with buffer_size (adjust as needed)\n",
    "    dataset = dataset.batch(batch_size=16)  # Batch size for training (adjust as needed)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory paths\n",
    "image_folder = 'normalized_images'\n",
    "mask_folder = 'normalized_masks'\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = load_data(image_folder, mask_folder)\n",
    "\n",
    "# Check the dataset shapes (optional)\n",
    "for images, masks in train_dataset.take(1):\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Mask batch shape:\", masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of GPUs available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "print(\"Is TensorFlow using GPU? \", tf.test.is_gpu_available())\n",
    "print(\"GPU Details: \", tf.config.experimental.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "unet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "unet.fit(train_dataset, epochs=5)  # Adjust the number of epochs as needed\n",
    "\n",
    "# Save the model\n",
    "unet.save('unet_model.h5')  # Save the model in .h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('unet_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = loaded_model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*accuracy))\n",
    "\n",
    "# Predict on a sample image\n",
    "sample_image = load_image('normalized_images/test.png')\n",
    "sample_image = tf.expand_dims(sample_image, 0)  # Add batch dimension\n",
    "prediction = loaded_model.predict(sample_image)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
